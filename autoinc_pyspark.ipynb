{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "With this mini project, you will exercise using Spark transformations to solve traditional\n",
    "MapReduce data problems. It demonstrates Spark having a significant advantage against\n",
    "Hadoop MapReduce framework, in both code simplicity and its in-memory processing\n",
    "performance, which best suit for the chain of MapReduce use cases.\n",
    "\n",
    "Consider an automobile tracking platform that keeps track of history of incidents after a new\n",
    "vehicle is sold by the dealer. Such incidents include further private sales, repairs and accident\n",
    "reports. This provides a good reference for second hand buyers to understand the vehicles they\n",
    "are interested in.\n",
    "\n",
    "# Objective:\n",
    "Find total number of accidents per make and year of the car.\n",
    "\n",
    "## Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-------------------------------------------------------------+\n",
      "|Column       |Type  |Info                                                         |\n",
      "+-------------+------+-------------------------------------------------------------+\n",
      "|incident_id  |INT   |                                                             |\n",
      "|incident_type|STRING|(I: initial sale, A: accident, R: repair)                    |\n",
      "|vin_number   |STRING|                                                             |\n",
      "|make         |STRING|(The brand of the car, only populated with incident type “I”)|\n",
      "|model        |STRING|(The model of the car, only populated with incident type “I”)|\n",
      "|year         |STRING|(The year of the car, only populated with incident type “I”) |\n",
      "|incident_date|DATE  |(Date of the incident occurrence)                            |\n",
      "|description  |STRING|                                                             |\n",
      "+-------------+------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = [('incident_id', 'INT',''), \\\n",
    "     ('incident_type','STRING','(I: initial sale, A: accident, R: repair)'), \\\n",
    "     ('vin_number','STRING',''), \\\n",
    "     ('make','STRING','(The brand of the car, only populated with incident type “I”)'), \\\n",
    "     ('model','STRING','(The model of the car, only populated with incident type “I”)'), \\\n",
    "     ('year','STRING','(The year of the car, only populated with incident type “I”)') ,\\\n",
    "     ('incident_date','DATE','(Date of the incident occurrence)'), \\\n",
    "     ('description','STRING','')]\n",
    "df_schema = spark.createDataFrame(s, \"Column: string, Type: string, Info: string\")\n",
    "df_schema.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Filter out accident incidents with make and year\n",
    "Since we only care about accident records, we should filter out records having incident type\n",
    "other than “I”. However, accident records don't carry make and year fields due to the design to\n",
    "remove redundancy. So our first step is propagating make and year info from record type I into\n",
    "all other record types.\n",
    "\n",
    "### 1.1 Read the input data CSV file\n",
    "Use the Spark context object to read the input file to create the input RDD:\n",
    "\n",
    "`sc = SparkContext(\"local\", \"My Application\")`\n",
    "<br>\n",
    "`raw_rdd = sc.textFile(\"data.csv\")`\n",
    "\n",
    "Because pyspark kernel for jupyter notebook automatically initializes SparkContext, etc - it's not required\n",
    "But in python script used by run.sh, the following commands are required to initialize the environment:\n",
    "\n",
    "`import pyspark`\n",
    "<br>\n",
    "`from pyspark.sql import SparkSession`\n",
    "<br>\n",
    "`from pyspark import SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------------------------------------+\n",
      "|Configuration                     |Value                                             |\n",
      "+----------------------------------+--------------------------------------------------+\n",
      "|spark.app.startTime               |1654788065112                                     |\n",
      "|spark.executor.id                 |driver                                            |\n",
      "|spark.driver.port                 |44203                                             |\n",
      "|spark.app.name                    |PySparkShell                                      |\n",
      "|spark.sql.catalogImplementation   |hive                                              |\n",
      "|spark.rdd.compress                |True                                              |\n",
      "|spark.driver.host                 |10.0.1.137                                        |\n",
      "|spark.app.id                      |local-1654788066024                               |\n",
      "|spark.serializer.objectStreamReset|100                                               |\n",
      "|spark.master                      |local[*]                                          |\n",
      "|spark.submit.pyFiles              |                                                  |\n",
      "|spark.submit.deployMode           |client                                            |\n",
      "|spark.sql.warehouse.dir           |file:/home/conner/SparkMiniProject/spark-warehouse|\n",
      "|spark.ui.showConsoleProgress      |true                                              |\n",
      "+----------------------------------+--------------------------------------------------+\n",
      "\n",
      "+-------------+------+-------------------------------------------------------------+\n",
      "|Column       |Type  |Info                                                         |\n",
      "+-------------+------+-------------------------------------------------------------+\n",
      "|incident_id  |INT   |                                                             |\n",
      "|incident_type|STRING|(I: initial sale, A: accident, R: repair)                    |\n",
      "|vin_number   |STRING|                                                             |\n",
      "|make         |STRING|(The brand of the car, only populated with incident type “I”)|\n",
      "|model        |STRING|(The model of the car, only populated with incident type “I”)|\n",
      "|year         |STRING|(The year of the car, only populated with incident type “I”) |\n",
      "|incident_date|DATE  |(Date of the incident occurrence)                            |\n",
      "|description  |STRING|                                                             |\n",
      "+-------------+------+-------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1,I,VXIO456XLBB630221,Nissan,Altima,2003,2002-05-08,Initial sales from TechMotors',\n",
       " '2,I,INU45KIOOPA343980,Mercedes,C300,2015,2014-01-01,Sold from EuroMotors',\n",
       " '3,A,VXIO456XLBB630221,,,,2014-07-02,Head on collision',\n",
       " '4,R,VXIO456XLBB630221,,,,2014-08-05,Repair transmission',\n",
       " '5,I,VOME254OOXW344325,Mercedes,E350,2015,2014-02-01,Sold from Carmax',\n",
       " '6,R,VOME254OOXW344325,,,,2015-02-06,Wheel allignment service',\n",
       " '7,R,VXIO456XLBB630221,,,,2015-01-01,Replace right head light',\n",
       " '8,I,EXOA00341AB123456,Mercedes,SL550,2016,2015-01-01,Sold from AceCars',\n",
       " '9,A,VOME254OOXW344325,,,,2015-10-01,Side collision',\n",
       " '10,R,VOME254OOXW344325,,,,2015-09-01,Changed tires',\n",
       " '11,R,EXOA00341AB123456,,,,2015-05-01,Repair engine',\n",
       " '12,A,EXOA00341AB123456,,,,2015-05-03,Vehicle rollover',\n",
       " '13,R,VOME254OOXW344325,,,,2015-09-01,Replace passenger side door',\n",
       " '14,I,UXIA769ABCC447906,Toyota,Camery,2017,2016-05-08,Initial sales from Carmax',\n",
       " '15,R,UXIA769ABCC447906,,,,2020-01-02,Initial sales from Carmax',\n",
       " '16,A,INU45KIOOPA343980,,,,2020-05-01,Side collision']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark kernel for jupyter notebook automatically initializes SparkContext variable sc\n",
    "spark.createDataFrame(sc.getConf().getAll(), \"Configuration: string, Value: string\").show(truncate = False)\n",
    "raw_rdd = sc.textFile(\"data.csv\")\n",
    "df_schema.show(truncate=False)\n",
    "raw_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Perform map operation\n",
    "We need to propagate make and year to the accident records (incident type A), using\n",
    "vin_number as the aggregate key. Therefore the map output key should be vin_number, value\n",
    "should be the make and year, along with the incident type. In Spark, in order to proceed with the\n",
    "“groupByKey” function, we need the map operation to produce PairRDD, with tuple type as each\n",
    "record.\n",
    "\n",
    "`vin_kv = raw_rdd.map(lambda x: extract_vin_key_value(x))`\n",
    "<br>\n",
    "`# Please implement method extract_vin_key_value()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VXIO456XLBB630221', ('I', 'Nissan', '2003')),\n",
       " ('INU45KIOOPA343980', ('I', 'Mercedes', '2015')),\n",
       " ('VXIO456XLBB630221', ('A', '', '')),\n",
       " ('VXIO456XLBB630221', ('R', '', '')),\n",
       " ('VOME254OOXW344325', ('I', 'Mercedes', '2015')),\n",
       " ('VOME254OOXW344325', ('R', '', '')),\n",
       " ('VXIO456XLBB630221', ('R', '', '')),\n",
       " ('EXOA00341AB123456', ('I', 'Mercedes', '2016')),\n",
       " ('VOME254OOXW344325', ('A', '', '')),\n",
       " ('VOME254OOXW344325', ('R', '', '')),\n",
       " ('EXOA00341AB123456', ('R', '', '')),\n",
       " ('EXOA00341AB123456', ('A', '', '')),\n",
       " ('VOME254OOXW344325', ('R', '', '')),\n",
       " ('UXIA769ABCC447906', ('I', 'Toyota', '2017')),\n",
       " ('UXIA769ABCC447906', ('R', '', '')),\n",
       " ('INU45KIOOPA343980', ('A', '', ''))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_vin_key_value(row):\n",
    "    auto = row.strip().split(\",\")\n",
    "    return (auto[2], (auto[1], auto[3], auto[5]))\n",
    "\n",
    "# Extract VIN for key,make,year,incident_type for value\n",
    "vin_kv = raw_rdd.map(lambda row: extract_vin_key_value(row))\n",
    "\n",
    "vin_kv.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Perform group aggregation to populate make and year to all the records\n",
    "Like the reducer in MapReduce framework, Spark provides a “groupByKey” function to achieve\n",
    "shuffle and sort in order to aggregate all records sharing the same key to the same groups.\n",
    "Within a group of vin_number, we need to iterate through all the records and find the one that\n",
    "has the make and year available and capture it in group level master info. As we filter and\n",
    "output accident records, those records need to be modified adding the master info that we\n",
    "captured in the first iteration.\n",
    "\n",
    "`enhance_make = vin_kv.groupByKey().flatMap(lambda kv: populate_make(kv[1]))`\n",
    "<br>\n",
    "`# Please implement method populate_make()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'Mercedes', '2015'],\n",
       " ['R', 'Mercedes', '2015'],\n",
       " ['A', 'Mercedes', '2015'],\n",
       " ['R', 'Mercedes', '2015'],\n",
       " ['R', 'Mercedes', '2015'],\n",
       " ['I', 'Mercedes', '2016'],\n",
       " ['R', 'Mercedes', '2016'],\n",
       " ['A', 'Mercedes', '2016'],\n",
       " ['I', 'Toyota', '2017'],\n",
       " ['R', 'Toyota', '2017'],\n",
       " ['I', 'Nissan', '2003'],\n",
       " ['A', 'Nissan', '2003'],\n",
       " ['R', 'Nissan', '2003'],\n",
       " ['R', 'Nissan', '2003'],\n",
       " ['I', 'Mercedes', '2015'],\n",
       " ['A', 'Mercedes', '2015']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def populate_make_printer(row):\n",
    "    print('NEW ROW\\n')\n",
    "    group_row = []\n",
    "    for i in row:\n",
    "        group_row.append(list(i))\n",
    "        print('i = '+str(i))\n",
    "    \n",
    "    print('group_row: '+str(group_row))\n",
    "    print('range(0, len(group_row)): '+str(range(0,len(group_row))))\n",
    "    \n",
    "    for j in range(0, len(group_row)):\n",
    "        print('j: ' + str(j))\n",
    "        print('j-1: ' + str(j-1))\n",
    "        print('group_row[j]: ' + str(group_row[j]))\n",
    "        print('group_row[j-1]: ' + str(group_row[j-1]))\n",
    "\n",
    "        make = group_row[j-1][1]\n",
    "        print('make: '+str(make))\n",
    "        year = group_row[j-1][2]\n",
    "        print('year: '+str(year))\n",
    "        \n",
    "        print('group_row[j][0]: ' + str(group_row[j][0]))\n",
    "        if group_row[j][0] != 'I':\n",
    "            print('group_row[j][0] NOT EQUAL I!\\n')\n",
    "            print('group_row[j][1]: ' + str(group_row[j][1]))\n",
    "            print('group_row[j][2]: ' + str(group_row[j][1]))\n",
    "            if group_row[j][1] == '' and group_row[j][2] == '':\n",
    "                print('group_row[j][1] NOT BLANK and group_row[j][2] == NOT BLANK!\\n')\n",
    "                group_row[j][1] = make\n",
    "                group_row[j][2] = year\n",
    "    \n",
    "    print('RETURN\\n')\n",
    "    return group_row\n",
    "\n",
    "def populate_make(row):\n",
    "    group_row = []\n",
    "    for i in row:\n",
    "        group_row.append(list(i))\n",
    "    for j in range(0, len(group_row)):\n",
    "        make = group_row[j-1][1]\n",
    "        year = group_row[j-1][2]\n",
    "        if group_row[j][0] != 'I':\n",
    "            if group_row[j][1] == '' and group_row[j][2] == '':\n",
    "                group_row[j][1] = make\n",
    "                group_row[j][2] = year\n",
    "    return group_row\n",
    "\n",
    "# Populate vehicle make and year for records missing these values\n",
    "enhance_make = vin_kv.groupByKey().flatMap(lambda row: populate_make(row[1]))\n",
    "enhance_make.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Count number of occurrence for accidents for the vehicle make and year\n",
    "\n",
    "### 2.1 Perform map operation\n",
    "The goal of this step is to count the number of records for each make and year combination,\n",
    "given the result we derived previously. The output key should be the combination of vehicle\n",
    "make and year. The value should be the count of 1.\n",
    "\n",
    "`make_kv = enhance_make.map(lambda x: extract_make_key_value(x))`\n",
    "<br>\n",
    "`# Please implement method extract_make_key_value()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Mercedes', '2015'), 0),\n",
       " (('Mercedes', '2015'), 0),\n",
       " (('Mercedes', '2015'), 1),\n",
       " (('Mercedes', '2015'), 0),\n",
       " (('Mercedes', '2015'), 0),\n",
       " (('Mercedes', '2016'), 0),\n",
       " (('Mercedes', '2016'), 0),\n",
       " (('Mercedes', '2016'), 1),\n",
       " (('Toyota', '2017'), 0),\n",
       " (('Toyota', '2017'), 0),\n",
       " (('Nissan', '2003'), 0),\n",
       " (('Nissan', '2003'), 1),\n",
       " (('Nissan', '2003'), 0),\n",
       " (('Nissan', '2003'), 0),\n",
       " (('Mercedes', '2015'), 0),\n",
       " (('Mercedes', '2015'), 1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_make_key_value(row):\n",
    "\n",
    "    if row[0] == 'A':\n",
    "        return (row[1], row[2]), 1\n",
    "    else:\n",
    "        return (row[1], row[2]), 0\n",
    "\n",
    "# Filter records that are not accident incidents\n",
    "make_kv = enhance_make.map(lambda x: extract_make_key_value(x))\n",
    "\n",
    "make_kv.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Aggregate the key and count the number of records in total per key\n",
    "Use Spark provided “reduceByKey” function to perform the sum of all the values (1) from each\n",
    "record. As a result, we get the make and year combination key along with its total record count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mercedes-2015,2', 'Mercedes-2016,1', 'Nissan-2003,1']\n"
     ]
    }
   ],
   "source": [
    "# Count accident incidents by make and year\n",
    "acc_make_year = make_kv.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "output = []\n",
    "for accident in acc_make_year.collect():\n",
    "    if accident[1] >= 1:\n",
    "        output.append('{}-{},{}'.format(accident[0][0], accident[0][1], accident[1]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Save the result to HDFS as text\n",
    "The output file should look similar to this.\n",
    "\n",
    "`Nissan-2003,1`\n",
    "<br>\n",
    "`BMW-2008,10`\n",
    "<br>\n",
    "`MERCEDES-2013,2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First to plain text file for sanity check\n",
    "file = open('output.txt', 'x')\n",
    "for acc in acc_make_year.collect():\n",
    "    file.write('{}-{},{}\\n'.format(acc[0][0], acc[0][1], acc[1]))\n",
    "file.close()\n",
    "\n",
    "# Now in HDFS file format\n",
    "out = []\n",
    "for acc in acc_make_year.collect():\n",
    "    out.append('{}-{},{}'.format(acc[0][0], acc[0][1], acc[1]))\n",
    "rdd_out = sc.parallelize(out)\n",
    "rdd_out.saveAsTextFile('HDFS_text_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My run.sh file consists of:\n",
    "    \n",
    "`#!/bin/sh`\n",
    "<br>\n",
    "`python autoinc_spark.py > execution_log.txt`\n",
    "<br>\n",
    "`echo '\\nsingle file results printed to output.txt'`\n",
    "<br>\n",
    "`echo '\\nresults saved as text file in HDFS format in HDFS_text_results folder'`\n",
    "<br>\n",
    "\n",
    "Running `./run.sh` produces file output.txt and partitions in HDFS_text folder containing the following records:\n",
    "\n",
    "`Mercedes-2015,2`\n",
    "<br>\n",
    "`Mercedes-2016,1`\n",
    "<br>\n",
    "`Toyota-2017,0`\n",
    "<br>\n",
    "`Nissan-2003,1`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
